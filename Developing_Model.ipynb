{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Developing_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/SwapnilSParkhe/Project-Image_Caption_Generation/blob/master/Developing_Model.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vKHosnAe9jk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Developing Model:\n",
        "\n",
        " - Building Model Data (ADS) and Architecture \n",
        " - Fitting built Model to Data\n",
        " - Evaluating Model Performance"
      ]
    },
    {
      "metadata": {
        "id": "qD71eYV--JpO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Checking GPU status**"
      ]
    },
    {
      "metadata": {
        "id": "kea9R2Q--Sxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "02ab0b18-f0d5-452c-a109-012ed715afeb"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6626033468801414995, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11288962663\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 13976021197597257283\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "sx-pKMxI9jk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Analytical Dataset (ADS) Creation"
      ]
    },
    {
      "metadata": {
        "id": "6IOEkIvT9jk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Image ID or Identifiers**"
      ]
    },
    {
      "metadata": {
        "id": "HCPKCkuW_D0k",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b57deaff-fd6a-4172-e98b-4595255e9c2f"
      },
      "cell_type": "code",
      "source": [
        "#Uploading relevant files from local to cloud (using google.colab lib)\n",
        "\n",
        "#Library for Uploading data from local to cloud\n",
        "from google.colab import files\n",
        "\n",
        "#Upload train image text\n",
        "files.upload()   #upload files \n",
        "\n",
        "#Upload valid image text\n",
        "files.upload()   #upload files\n",
        "\n",
        "#Upload test image text\n",
        "files.upload()   #upload files"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9391a030-bcbc-4aaf-ab94-48dad8d9f572\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9391a030-bcbc-4aaf-ab94-48dad8d9f572\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3598e64c-1a82-4c57-aa60-463d651d7831\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3598e64c-1a82-4c57-aa60-463d651d7831\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-04273704-9b1c-4c80-9b0e-e131825e08e2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-04273704-9b1c-4c80-9b0e-e131825e08e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "g8KSTWjR9jk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing file: reading the content into Py file\n",
        "def import_file(input_file):\n",
        "    file=open(input_file,'r')   #creating a bridge btwn OS and Py files\n",
        "    content=file.read()   #reading content via the bridge\n",
        "    file.close()   #closing the bridge\n",
        "    return content\n",
        "\n",
        "imported_train=import_file('Flickr_8k.trainImages.txt') \n",
        "imported_valid=import_file('Flickr_8k.devImages.txt') \n",
        "imported_test=import_file('Flickr_8k.testImages.txt')    \n",
        "\n",
        "#Creating a set of image-IDs\n",
        "def create_img_set(file):\n",
        "    imgID_set=list()\n",
        "    for item in file.split('\\n'):   #accessing line by line\n",
        "        if len(item)<1:   #rejecting empty spaces\n",
        "            continue\n",
        "        imgID=item.split('.')[0]   #only taking imgID (rejecting 'jpg')\n",
        "        imgID_set.append(imgID)   #appending imgIDs to imgID_set\n",
        "    return set(imgID_set)\n",
        "\n",
        "imgID_trainset=create_img_set(imported_train)\n",
        "imgID_validset=create_img_set(imported_valid)\n",
        "imgID_testset=create_img_set(imported_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-KVDSdl9jlC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing previously created files (from PreprocessingData NoteBook): Img desc and Img features**"
      ]
    },
    {
      "metadata": {
        "id": "PjiYF0TQARnm",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "59db5ba0-c30c-4a56-ea75-ba1f9f3e6cda"
      },
      "cell_type": "code",
      "source": [
        "#Uploading relevant files from local to cloud (using google.colab lib)\n",
        "#Note: Preprocessing Desc and Feat was done solely on training data files\n",
        "\n",
        "#Library for Uploading data from local to cloud\n",
        "from google.colab import files\n",
        "\n",
        "#Upload cleaned organised text file (from Text precprosssing step)\n",
        "files.upload()   #upload files \n",
        "\n",
        "#Upload featuresl file (from Image preprocessing step)\n",
        "files.upload()   #upload files"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a22067f-1cd9-4ee3-a0d1-674898d7743f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0a22067f-1cd9-4ee3-a0d1-674898d7743f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving cln_orgnse_text.txt to cln_orgnse_text.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-43e411dd-7966-4c8f-a800-8516f4ca99d8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-43e411dd-7966-4c8f-a800-8516f4ca99d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.pkl to features.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9h3hx2SV9jlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing image desc files for this image data germane to training set\n",
        "def import_prepro_desc(prepro_file, dataset):\n",
        "    file=import_file(prepro_file)\n",
        "    desc=dict()\n",
        "    for item in file.split('\\n'):\n",
        "        tokens=item.split()   #splitting by whitespaces\n",
        "        image_ID,image_desc=tokens[0],tokens[1:]   #separating ID, desc\n",
        "        if image_ID in dataset:   #inner join imgID & training imgID \n",
        "            if image_ID not in desc:   #new list for new image_ID key \n",
        "                desc[image_ID]=list()\n",
        "            desc_='start ' + ' '.join(image_desc)+' end'   #wrap in tokens\n",
        "            desc[image_ID].append(desc_)\n",
        "    return desc\n",
        "\n",
        "desc_train=import_prepro_desc('cln_orgnse_text.txt',imgID_trainset)\n",
        "desc_valid=import_prepro_desc('cln_orgnse_text.txt',imgID_validset)\n",
        "desc_test=import_prepro_desc('cln_orgnse_text.txt',imgID_testset)\n",
        "\n",
        "#Importing image features for this image data germane to training set\n",
        "from pickle import load\n",
        "def import_features(feature_file, dataset):\n",
        "    all_features = load(open(feature_file, 'rb'))  #load all features\n",
        "    features = {k: all_features[k] for k in dataset} #inner join\n",
        "    return features\n",
        "\n",
        "feature_train=import_features('features.pkl',imgID_trainset) #used later\n",
        "feature_valid=import_features('features.pkl',imgID_validset) #used later\n",
        "feature_test=import_features('features.pkl',imgID_testset) #used later"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVVJsEMB9jlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training data manipulations: Creating a custom Tokeizer function: Tokenizing descriptions**"
      ]
    },
    {
      "metadata": {
        "id": "xSizh-do9jlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "09e0e104-a209-4364-8f08-907c48a81eed"
      },
      "cell_type": "code",
      "source": [
        "#Creating a simple list of desc from dict of desc\n",
        "def dict2list(input_dict):\n",
        "    desc_list=list()\n",
        "    for key in input_dict.keys():\n",
        "        [desc_list.append(d) for d in input_dict[key]]\n",
        "    return desc_list\n",
        "\n",
        "desc_train_list=dict2list(desc_train)\n",
        "\n",
        "#tokeinizing (could be improved by filetring english stopwords later)\n",
        "#Note: turning each text into sequence of integers (integer: token ID)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from pickle import dump\n",
        "def tokenize(input_list):\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(input_list)\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer=tokenize(desc_train_list) #to be used later\n",
        "vocab_size=len(tokenizer.word_index)+1 #to be used later\n",
        "print(\"Vocab Size:\",vocab_size)\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb')) #to be used later\n",
        "\n",
        "#Length of the description with the most words\n",
        "def max_length(desc_list):\n",
        "    max_len=max([len(item.split()) for item in desc_list])\n",
        "    return max_len\n",
        "max_length = max_length(desc_train_list) #to be used later\n",
        "print('Description Length', max_length)\n",
        "\n",
        "#Longest desc check\n",
        "def longest_desc(desc_list):\n",
        "    max_length=max([len(item.split()) for item in desc_list])\n",
        "    print(\"Max_len:\",max_length)\n",
        "    print(\"Desc:\", [item for item in desc_list if len(item.split())==max_length])\n",
        "\n",
        "longest_desc(desc_train_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 7264\n",
            "Description Length 33\n",
            "Max_len: 33\n",
            "Desc: ['start an man wearing green sweatshirt and blue vest is holding up dollar bills in front of his face while standing on busy sidewalk in front of group of men playing instruments end']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IvQh6N5V9jlN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**LSTM's Analytical Dataset: Input(ImageID and Seq_item)-Ouput(SeqWord) data**"
      ]
    },
    {
      "metadata": {
        "id": "RHUAPoyW9jlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating ADS for LSTM: Input(Image_ID and Seq_item)-Ouput(SeqWord)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "def create_ADS(tokenizer, max_length, desc_list, img):\n",
        "    X_img_ID, X_desc_item, y=list(), list(), list()\n",
        "    for desc in desc_list:\n",
        "        seq=tokenizer.texts_to_sequences([desc])[0] #encoding seq\n",
        "        for i in range(1,len(seq)):#split seq into multi X,y pairs\n",
        "            in_seq, out_seq=seq[:i], seq[i] #desc input-output pair\n",
        "            in_seq=pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq=to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            X_img_ID.append(img) #appending  img IDs\n",
        "            X_desc_item.append(in_seq)  #multi X-y pairs encoding\n",
        "            y.append(out_seq)   #oneHot encoded version of output word\n",
        "    return np.array(X_img_ID), np.array(X_desc_item), np.array(y)\n",
        "\n",
        "#Progressive Data Loading: Generate data (yield one photo’s data/batch) \n",
        "#Note: intended to be used in a call to model.fit_generator()\n",
        "def generate_data(tokenizer, max_length, desc_dict, img):\n",
        "    while 1:   #loop for ever over images\n",
        "        for key, desc_list in desc_dict.items(): #access image feature\n",
        "            img_ = img[key][0]  #image ID\n",
        "            in_img,in_seq,out_word=create_ADS(tokenizer,\n",
        "                                              max_length,\n",
        "                                              desc_list,img_)\n",
        "            yield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJmJ8lWb9jlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Model Architecture (Merge Model of Embeddings+LSTMs with CNN penultimate layer)\n",
        "**Note:** Combines both the encoded form (features) of the image input with the encoded form (context) of the text description generated so far; Combination of these two encoded inputs is then used by a very simple decoder model to generate the next word in the sequence"
      ]
    },
    {
      "metadata": {
        "id": "hCoFn7219jlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Merge Model of Image Captioning](https://i.pinimg.com/originals/35/8b/dc/358bdc11e71f8c78632560c7c819919d.png)"
      ]
    },
    {
      "metadata": {
        "id": "Cg8uJDkn9jlS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing relevant libraries**"
      ]
    },
    {
      "metadata": {
        "id": "flMCXHs69jlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dropout, Dense #feat. encoding\n",
        "from keras.layers import Embedding, Dropout, LSTM #desc. encoding\n",
        "from keras.layers.merge import add #decoding\n",
        "from keras.models import Model #Model-Input-Output architecture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVR6G_jU9jlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "6ef441d3-bbb9-4f62-ce99-fca05b81fb28"
      },
      "cell_type": "code",
      "source": [
        "def build_model_arch(vocab_size, max_length):\n",
        "    #Encoder Models (Img-Feat and Desc Encoding)\n",
        "    #1.Image feature extractor model\n",
        "    feat_input=Input(shape=(4096,))\n",
        "    feat_1=Dropout(0.5)(feat_input)\n",
        "    feat_2=Dense(300, activation='relu')(feat_1)\n",
        "\n",
        "    #2.Embedding+LSTM sequence model\n",
        "    desc_input=Input(shape=(max_length,))\n",
        "    desc_1=Embedding(vocab_size, 300, mask_zero=True)(desc_input)\n",
        "    desc_2=Dropout(0.5)(desc_1)\n",
        "    desc_3=LSTM(300)(desc_2)\n",
        "\n",
        "    #Decoder Model ('adding' above encoding model layers; with FFNs)\n",
        "    deco_1=add([feat_2, desc_3]) #adding element wise for both vectors\n",
        "    deco_2=Dense(300, activation='relu')(deco_1)\n",
        "    output=Dense(vocab_size, activation='softmax')(deco_2)\n",
        "\n",
        "    #Creating Model-Input-Output architecture; Compiling (with loss, opt.)\n",
        "    model=Model(inputs=[feat_input, desc_input], outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    #Summarizing and Plotting model\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model = build_model_arch(vocab_size, max_length)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           (None, 33)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 33, 300)      2179200     input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 4096)         0           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 33, 300)      0           embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 300)          1229100     dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 300)          721200      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 300)          0           dense_13[0][0]                   \n",
            "                                                                 lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 300)          90300       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 7264)         2186464     dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,406,264\n",
            "Trainable params: 6,406,264\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N86jVNzm9jla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fitting Model to train data (validation data to balance bias-variance)"
      ]
    },
    {
      "metadata": {
        "id": "0AkfJjHm9jlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "b320db43-04c1-4574-8929-ed2feb08c7b6"
      },
      "cell_type": "code",
      "source": [
        "#Defining checkpoint callback; specifying model hyperparams\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = 'best_model_weights.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min')\n",
        "epochs_=10\n",
        "steps_train=len(desc_train)  #steps=N/batch_size\n",
        "steps_valid=len(desc_valid)  #steps=N/batch_size\n",
        "\n",
        "#Fitting model to generated data (along side validation loss checks)\n",
        "generated_data_train=generate_data(tokenizer, max_length, \n",
        "                                   desc_train, feature_train)\n",
        "generated_data_valid=generate_data(tokenizer, max_length, \n",
        "                                   desc_valid, feature_valid)\n",
        "model.fit_generator(generated_data_train, epochs=epochs_,\n",
        "                    steps_per_epoch=steps_train,\n",
        "                    validation_data=generated_data_valid,\n",
        "                    validation_steps=steps_valid,\n",
        "                    callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5999/6000 [============================>.] - ETA: 0s - loss: 4.6177"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6000/6000 [==============================] - 767s 128ms/step - loss: 4.6180 - val_loss: 4.1241\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.12408, saving model to best_model_weights.h5\n",
            "Epoch 2/10\n",
            " 297/6000 [>.............................] - ETA: 11:41 - loss: 4.0121"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 770s 128ms/step - loss: 3.8706 - val_loss: 3.9286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00002: val_loss improved from 4.12408 to 3.92858, saving model to best_model_weights.h5\n",
            "Epoch 3/10\n",
            " 567/6000 [=>............................] - ETA: 11:10 - loss: 3.7492"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 769s 128ms/step - loss: 3.6224 - val_loss: 3.8648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00003: val_loss improved from 3.92858 to 3.86475, saving model to best_model_weights.h5\n",
            "Epoch 4/10\n",
            " 567/6000 [=>............................] - ETA: 11:12 - loss: 3.5737"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 766s 128ms/step - loss: 3.4752 - val_loss: 3.8414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00004: val_loss improved from 3.86475 to 3.84139, saving model to best_model_weights.h5\n",
            "Epoch 5/10\n",
            " 567/6000 [=>............................] - ETA: 11:01 - loss: 3.4541"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 770s 128ms/step - loss: 3.3766 - val_loss: 3.8387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00005: val_loss improved from 3.84139 to 3.83872, saving model to best_model_weights.h5\n",
            "Epoch 6/10\n",
            " 567/6000 [=>............................] - ETA: 11:17 - loss: 3.3488"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 769s 128ms/step - loss: 3.3074 - val_loss: 3.8348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00006: val_loss improved from 3.83872 to 3.83478, saving model to best_model_weights.h5\n",
            "Epoch 7/10\n",
            " 567/6000 [=>............................] - ETA: 11:06 - loss: 3.3023"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 773s 129ms/step - loss: 3.2571 - val_loss: 3.8716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00007: val_loss did not improve from 3.83478\n",
            "Epoch 8/10\n",
            " 751/6000 [==>...........................] - ETA: 10:54 - loss: 3.2741"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 768s 128ms/step - loss: 3.2166 - val_loss: 3.8892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00008: val_loss did not improve from 3.83478\n",
            "Epoch 9/10\n",
            " 751/6000 [==>...........................] - ETA: 10:42 - loss: 3.2411"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4645/6000 [======================>.......] - ETA: 2:45 - loss: 3.1780"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xrdGHY2wVDaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#Training manually (if above code hangs or get stuck)\n",
        "epochs=20\n",
        "steps=len(desc_train)  #steps=N/batch_size\n",
        "for epoch in range(epochs):\n",
        "    generated_data_train=generate_data(tokenizer, max_length, \n",
        "                                       desc_train, feature_train)\n",
        "    generated_data_valid=generate_data(tokenizer, max_length, \n",
        "                                       desc_valid, feature_valid)\n",
        "    model.fit_generator(generated_data_train,steps_per_epoch=steps, \n",
        "                        validation_data= generated_data_valid,\n",
        "                        epochs=1, verbose=1)\n",
        "    model.save('model_' + str(epoch) + '.h5')\n",
        "####################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEr1X_x-oI5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model (on validaiton data using BLEAU score)"
      ]
    },
    {
      "metadata": {
        "id": "ZLV4inNtobmG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Generating description for a photo using the trained model (for a given tokenizer on train data)**\n",
        "\n",
        "Note: passing in the start description token ‘start_seq‘, generating one word, then calling the model recursively with generated words as input until the end of sequence token is reached ‘end_seq‘ or the maximum description length is reached"
      ]
    },
    {
      "metadata": {
        "id": "Txv_pUQcoICb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Mapping an integer prediction back to a word\n",
        "#Note: Using the same tokeniser used for train data\n",
        "def intID_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index==integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "#Generating a desc for an image using trained model\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def generate_desc(model, tokenizer, image, max_length):\n",
        "    in_text='start_seq'   #seeding the generation process\n",
        "    for i in range(max_length):\n",
        "        seq=tokenizer.texts_to_sequences([in_text])[0] #encoding txt2int\n",
        "        seq=pad_sequences([seq], maxlen=max_length) #padding seq\n",
        "        pred=model.predict([image,seq], verbose=0)  #predict using model\n",
        "        pred=argmax(pred)   #prob to integer ID conversion\n",
        "        word=intID_to_word(pred, tokenizer) #intID to word mapping\n",
        "        if word is None:\n",
        "            break   #stop if cant map word\n",
        "        in_text += ' ' + word  #append as input to generate next word\n",
        "        if word=='end_seq':\n",
        "            break   #stop if end of seq\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnmtWPnXojwE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Evaluating model**\n",
        "\n",
        "Note: for a given set of photo desc and photo features (on the validation data, could use test data later)"
      ]
    },
    {
      "metadata": {
        "id": "8ZTzXkT2qp0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing the nltk librarie's BLEU score evaluator\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "#Function for Evaluating model\n",
        "def evaluate_model(model, desc_dict, image, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    for key, desc_list in desc_dict.items():\n",
        "        pred_=generate_desc(model, tokenizer, image[key], max_length)\n",
        "        act_=[desc.split() for desc in desc_list]\n",
        "        predicted.append(pred_.split())\n",
        "        actual.append(act_)\n",
        "    #calculating BLEU score\n",
        "    print('BLEU-1:%f' % corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\n",
        "    print('BLEU-2:%f' % corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))\n",
        "    print('BLEU-3:%f' % corpus_bleu(actual,predicted,weights=(0.3,0.3,0.3,0)))\n",
        "    print('BLEU-4:%f' % corpus_bleu(actual,predicted,\n",
        "                                    weights=(0.25,0.25,0.25,0.25)))\n",
        "    \n",
        "#Loading or using best saved model\n",
        "from keras.models import load_model\n",
        "best_model = load_model('best_model_weights.h5')\n",
        "\n",
        "#Checking Performance on both train and valid data\n",
        "print('For training data:\\n', evaluate_model(best_model,desc_train,feature_train,tokenizer,max_length))\n",
        "print('\\n')\n",
        "print('For valid data:\\n', evaluate_model(best_model,desc_valid,feature_valid,tokenizer,max_length))\n",
        "print('\\n')\n",
        "print('For test data:\\n', evaluate_model(best_model,desc_test,feature_test,tokenizer,max_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T-aav5nG7R9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Saving and downloading relevant files for future use**"
      ]
    },
    {
      "metadata": {
        "id": "w7k4HE651npN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Tokenizer (based on training data)\n",
        "files.download('tokenizer.pkl')\n",
        "\n",
        "#Best model (weights)\n",
        "files.download('best_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvFqPQ3Fe85D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwiZTwoIe9YI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}